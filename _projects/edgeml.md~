---
title: "Machine Learning on the Edge"
excerpt: "A repository to train machine learning models for deployment on resource-constrained devices"
collection: "projects"
permalink: /projects/accelerated
date: 2017-12-22
---

We study optimization problems of the form:
\begin{equation}
e = mc^2
\end{equation}
One of the algorithms for optimizing under L0 (sparsity constraints) is to alternatingly perform a gradient descent and a thresholding step. This is called Iterative Hard Thresholding (IHT). We propose a new algorithm for IHT which does accelerated gradient descent instead of vanilla gradient descent. We are looking to prove guarantees for this algorithm. 
{: .text-justify}

